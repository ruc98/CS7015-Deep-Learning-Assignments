{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.utils.data as data_utils\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HYPERPARAMETERS ###\n",
    "dataset_no = 1  # 1 for color, 2 for BnW\n",
    "\n",
    "if dataset_no==1:\n",
    "    convergence = 1e-7\n",
    "    batch_size = 4\n",
    "    learning_rate = 1e-3\n",
    "    nodes_l1 = 200  # 64,128,256,512  ,best 512\n",
    "    nodes_l2 = 100  # 64,128,256,384 ,best 384\n",
    "    nodes_l3 = 30   # 32,64,128,256, best 256\n",
    "\n",
    "if dataset_no==2:\n",
    "    convergence = 1e-7\n",
    "    batch_size = 128\n",
    "    learning_rate = 1e-2\n",
    "    nodes_l1 = 512  # 64,128,256,512  ,best 512\n",
    "    nodes_l2 = 384  # 64,128,256,384 ,best 384\n",
    "    nodes_l3 = 256   # 32,64,128,256, best 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data loading ###\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(0)\n",
    "\n",
    "if dataset_no ==1:\n",
    "    data1=np.zeros(shape=(1596,829))\n",
    "    ji=0\n",
    "    labels = ['coast','forest','highway','street','tallbuilding']\n",
    "    for idx, label in enumerate(labels):\n",
    "        path = 'Features/'+label\n",
    "        for file in os.listdir(path):\n",
    "            current = os.path.join(path, file)\n",
    "            in1=open(current)\n",
    "            l1 = in1.read().strip().split(\"\\n\")\n",
    "            l2=[]\n",
    "            for i in l1:\n",
    "                l2=l2+i.split(\" \")\n",
    "            l2.append(idx)\n",
    "            l2=np.array(l2)\n",
    "            l2=np.float_(l2)\n",
    "            data1[ji]=l2\n",
    "            ji=ji+1\n",
    "\n",
    "if dataset_no ==2:\n",
    "    datafile = 'BnW/7/data.csv'\n",
    "    data1 = pd.read_csv(datafile, encoding = \"UTF-8\")\n",
    "    data1=data1.values\n",
    "            \n",
    "np.random.shuffle(data1)\n",
    "train_test_split = int(0.8*data1.shape[0]) # 80% traindata\n",
    "data_train = data1[:train_test_split]\n",
    "data_test  = data1[train_test_split:]\n",
    "\n",
    "unlabeled_labeled_split = int(0.7*data1.shape[0]) # 70% of traindata as unlabeled data\n",
    "data_labeled = data1[unlabeled_labeled_split:]\n",
    "data_train = data_labeled\n",
    "\n",
    "targets=torch.Tensor(np.ravel(data_train[:,-1]))\n",
    "features=torch.Tensor(data_train[:,:-1])\n",
    "targets_t=torch.Tensor(np.ravel(data_test[:,-1]))\n",
    "features_t=torch.Tensor(data_test[:,:-1])\n",
    "\n",
    "dataset = data_utils.TensorDataset(features, targets)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "input_dim = data1[:,:-1].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model definition ###\n",
    "class autoencoder1(nn.Module):                # autoencoder 1 model\n",
    "    def __init__(self):\n",
    "        super(autoencoder1, self).__init__()\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, nodes_l1),\n",
    "            nn.ReLU(True))\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.Linear(nodes_l1, input_dim),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def bottle(self,x):                     # forward pass through encoder only\n",
    "            x=self.encoder1(x)\n",
    "            return x\n",
    "\n",
    "        \n",
    "    def forward(self, x):                   # forward pass through encoder and decoder\n",
    "            x = self.encoder1(x)\n",
    "            x = self.decoder1(x)\n",
    "            return x\n",
    "\n",
    "        \n",
    "class autoencoder2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder2, self).__init__()\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Linear(nodes_l1,nodes_l2),\n",
    "            nn.ReLU(True))\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.Linear(nodes_l2,nodes_l1),\n",
    "            nn.ReLU(True))\n",
    "\n",
    "    def bottle(self,x):\n",
    "            x=self.encoder2(x)\n",
    "            return x\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "            x = self.encoder2(x)\n",
    "            x = self.decoder2(x)\n",
    "            return x\n",
    "\n",
    "        \n",
    "class autoencoder3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder3, self).__init__()\n",
    "        self.encoder3 = nn.Sequential(\n",
    "            nn.Linear(nodes_l2,nodes_l3),\n",
    "            nn.ReLU(True))\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.Linear(nodes_l3,nodes_l2),\n",
    "            nn.ReLU(True))\n",
    "\n",
    "    def bottle(self,x):\n",
    "            x=self.encoder3(x)\n",
    "            return x\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "            x = self.encoder3(x)\n",
    "            x = self.decoder3(x)\n",
    "            return x\n",
    "\n",
    "model1 = autoencoder1()\n",
    "model2 = autoencoder2()\n",
    "model3 = autoencoder3()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.load_state_dict(torch.load('./autoencoder1.pth'))\n",
    "model2.load_state_dict(torch.load('./autoencoder2.pth'))\n",
    "model3.load_state_dict(torch.load('./autoencoder3.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SAE, self).__init__()\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, nodes_l1),\n",
    "            nn.ReLU(True))\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Linear(nodes_l1,nodes_l2),\n",
    "            nn.ReLU(True))\n",
    "        self.encoder3 = nn.Sequential(\n",
    "            nn.Linear(nodes_l2,nodes_l3),\n",
    "            nn.ReLU(True))\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.Linear(nodes_l3, 5),\n",
    "            nn.Softmax(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder1(x)\n",
    "        x = self.encoder2(x)\n",
    "        x = self.encoder3(x)\n",
    "        x = self.final_layer(x)\n",
    "        return x\n",
    "\n",
    "sae_model = SAE()\n",
    "# classif==sae_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_model.encoder1.load_state_dict(model1.encoder1.state_dict(), strict=True)\n",
    "sae_model.encoder2.load_state_dict(model2.encoder2.state_dict(), strict=True)\n",
    "sae_model.encoder3.load_state_dict(model3.encoder3.state_dict(), strict=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 1.8241, 0.2088, 0.2156\n",
      "2, 1.7372, 0.3152, 0.3344\n",
      "3, 1.6347, 0.3612, 0.3719\n",
      "4, 1.5658, 0.4092, 0.4188\n",
      "5, 1.5589, 0.4196, 0.4281\n",
      "6, 1.5511, 0.4489, 0.4469\n",
      "7, 1.5469, 0.4885, 0.4844\n",
      "8, 1.5460, 0.5198, 0.5156\n",
      "9, 1.5451, 0.5136, 0.5125\n",
      "10, 1.5490, 0.5428, 0.5469\n",
      "11, 1.5485, 0.5574, 0.5625\n",
      "12, 1.5500, 0.5783, 0.5750\n",
      "13, 1.5550, 0.5595, 0.5563\n",
      "14, 1.5524, 0.5762, 0.5781\n",
      "15, 1.5467, 0.5971, 0.6000\n",
      "16, 1.5428, 0.6305, 0.6312\n",
      "17, 1.5438, 0.6430, 0.6406\n",
      "18, 1.5382, 0.6660, 0.6625\n",
      "19, 1.5290, 0.6743, 0.6719\n",
      "20, 1.5282, 0.6722, 0.6750\n",
      "21, 1.5269, 0.6743, 0.6750\n",
      "22, 1.5294, 0.6827, 0.6813\n",
      "23, 1.5262, 0.6848, 0.6844\n",
      "24, 1.5307, 0.6806, 0.6781\n",
      "25, 1.5245, 0.6931, 0.6937\n",
      "26, 1.5370, 0.6889, 0.7000\n",
      "27, 1.5685, 0.6806, 0.6781\n",
      "28, 1.5714, 0.6159, 0.6188\n",
      "29, 1.5679, 0.6722, 0.6781\n",
      "30, 1.5706, 0.6242, 0.6344\n",
      "31, 1.5438, 0.6868, 0.7000\n",
      "32, 1.5231, 0.6931, 0.7094\n",
      "33, 1.5303, 0.6681, 0.6844\n",
      "34, 1.5246, 0.6931, 0.7063\n",
      "35, 1.5350, 0.6973, 0.7156\n",
      "36, 1.5225, 0.6889, 0.7094\n",
      "37, 1.5283, 0.6994, 0.7188\n",
      "38, 1.5306, 0.7077, 0.7188\n",
      "39, 1.5340, 0.7119, 0.7188\n",
      "40, 1.5319, 0.7140, 0.7188\n",
      "41, 1.5384, 0.7140, 0.7188\n",
      "42, 1.5525, 0.7223, 0.7219\n",
      "43, 1.5254, 0.7203, 0.7281\n",
      "44, 1.5335, 0.7098, 0.7156\n",
      "45, 1.5249, 0.7161, 0.7219\n",
      "46, 1.5376, 0.7307, 0.7312\n",
      "47, 1.5284, 0.7223, 0.7219\n",
      "48, 1.5442, 0.6994, 0.7094\n",
      "49, 1.5519, 0.7265, 0.7281\n",
      "50, 1.5261, 0.7265, 0.7281\n",
      "51, 1.5408, 0.7140, 0.7281\n",
      "52, 1.5385, 0.7223, 0.7250\n",
      "53, 1.5385, 0.7182, 0.7250\n",
      "54, 1.5232, 0.7140, 0.7250\n",
      "55, 1.5305, 0.6973, 0.7125\n",
      "56, 1.5431, 0.7307, 0.7281\n",
      "57, 1.5294, 0.7328, 0.7312\n",
      "58, 1.5341, 0.7077, 0.7250\n",
      "59, 1.5261, 0.7161, 0.7219\n",
      "60, 1.5311, 0.7203, 0.7281\n",
      "61, 1.5316, 0.7077, 0.7188\n",
      "62, 1.5232, 0.7161, 0.7250\n",
      "63, 1.5270, 0.7098, 0.7250\n",
      "64, 1.5234, 0.7328, 0.7406\n",
      "65, 1.5400, 0.7370, 0.7375\n",
      "66, 1.5268, 0.7349, 0.7344\n",
      "67, 1.5309, 0.7328, 0.7375\n",
      "68, 1.5291, 0.7286, 0.7406\n",
      "69, 1.5251, 0.7307, 0.7406\n",
      "70, 1.5268, 0.7370, 0.7406\n",
      "71, 1.5269, 0.7349, 0.7375\n",
      "72, 1.5298, 0.7370, 0.7375\n",
      "73, 1.5369, 0.7370, 0.7375\n",
      "74, 1.5359, 0.7370, 0.7375\n",
      "75, 1.5331, 0.7370, 0.7375\n",
      "76, 1.5361, 0.7390, 0.7406\n",
      "77, 1.5335, 0.7390, 0.7406\n",
      "78, 1.5364, 0.7390, 0.7406\n",
      "79, 1.5316, 0.7390, 0.7406\n",
      "80, 1.5262, 0.7370, 0.7406\n",
      "81, 1.5308, 0.7349, 0.7438\n",
      "82, 1.5475, 0.7411, 0.7438\n",
      "83, 1.5301, 0.7411, 0.7469\n",
      "84, 1.5340, 0.7432, 0.7500\n",
      "85, 1.5276, 0.7432, 0.7469\n",
      "86, 1.5330, 0.7432, 0.7469\n",
      "87, 1.5316, 0.7453, 0.7500\n",
      "88, 1.5272, 0.7453, 0.7500\n",
      "89, 1.5294, 0.7453, 0.7500\n",
      "90, 1.5296, 0.7453, 0.7500\n",
      "91, 1.5291, 0.7453, 0.7500\n",
      "92, 1.5289, 0.7453, 0.7500\n",
      "93, 1.5364, 0.7453, 0.7500\n",
      "94, 1.5347, 0.7453, 0.7500\n",
      "95, 1.5289, 0.7453, 0.7500\n",
      "96, 1.5290, 0.7453, 0.7500\n",
      "97, 1.5291, 0.7453, 0.7500\n",
      "98, 1.5355, 0.7453, 0.7500\n",
      "99, 1.5283, 0.7453, 0.7500\n",
      "100, 1.5292, 0.7432, 0.7500\n",
      "101, 1.5319, 0.7432, 0.7500\n",
      "102, 1.5413, 0.7474, 0.7531\n",
      "103, 1.5348, 0.7453, 0.7500\n",
      "104, 1.5474, 0.7474, 0.7531\n",
      "105, 1.5331, 0.7474, 0.7531\n",
      "106, 1.5236, 0.7474, 0.7531\n",
      "107, 1.5291, 0.7474, 0.7531\n",
      "108, 1.5328, 0.7474, 0.7531\n",
      "109, 1.5253, 0.7495, 0.7531\n",
      "110, 1.5282, 0.7495, 0.7531\n",
      "111, 1.5246, 0.7495, 0.7531\n",
      "112, 1.5215, 0.7495, 0.7531\n",
      "113, 1.5229, 0.7516, 0.7531\n",
      "114, 1.5256, 0.7516, 0.7531\n",
      "115, 1.5223, 0.7516, 0.7531\n",
      "116, 1.5220, 0.7516, 0.7531\n",
      "117, 1.5223, 0.7516, 0.7531\n",
      "118, 1.5267, 0.7537, 0.7562\n",
      "119, 1.5270, 0.7537, 0.7562\n",
      "120, 1.5212, 0.7537, 0.7562\n",
      "121, 1.5208, 0.7537, 0.7562\n",
      "122, 1.5203, 0.7537, 0.7562\n",
      "123, 1.5197, 0.7537, 0.7562\n",
      "124, 1.5188, 0.7537, 0.7562\n",
      "125, 1.5327, 0.7474, 0.7469\n",
      "126, 1.5339, 0.7474, 0.7469\n",
      "127, 1.5255, 0.7537, 0.7562\n",
      "128, 1.5556, 0.7537, 0.7562\n",
      "129, 1.5257, 0.7537, 0.7562\n",
      "130, 1.5343, 0.7537, 0.7562\n",
      "131, 1.5536, 0.7537, 0.7562\n",
      "132, 1.5464, 0.7537, 0.7562\n",
      "133, 1.5329, 0.7537, 0.7562\n",
      "134, 1.5253, 0.7537, 0.7562\n",
      "135, 1.5268, 0.7537, 0.7562\n",
      "136, 1.5242, 0.7537, 0.7562\n",
      "137, 1.5254, 0.7537, 0.7562\n",
      "138, 1.5248, 0.7537, 0.7562\n",
      "139, 1.5217, 0.7537, 0.7562\n",
      "140, 1.5163, 0.7537, 0.7562\n",
      "141, 1.1180, 0.8434, 0.8469\n",
      "142, 0.9848, 0.8497, 0.8531\n",
      "143, 0.9244, 0.8622, 0.8625\n",
      "144, 0.9270, 0.8601, 0.8594\n",
      "145, 0.9185, 0.8914, 0.8812\n",
      "146, 0.9180, 0.8852, 0.8781\n",
      "147, 0.9617, 0.8789, 0.8750\n",
      "148, 0.9072, 0.9040, 0.8906\n",
      "149, 0.9694, 0.9019, 0.8875\n",
      "150, 0.9279, 0.8914, 0.8938\n",
      "151, 0.9057, 0.9019, 0.8938\n",
      "152, 0.9117, 0.9144, 0.9062\n",
      "153, 0.9194, 0.9207, 0.9125\n",
      "154, 0.9070, 0.9040, 0.9062\n",
      "155, 0.9969, 0.8413, 0.8531\n",
      "156, 1.5452, 0.6848, 0.6781\n",
      "157, 0.9083, 0.9040, 0.9031\n",
      "158, 0.9139, 0.8727, 0.8656\n",
      "159, 0.9061, 0.9290, 0.9219\n",
      "160, 0.9057, 0.9165, 0.9156\n",
      "161, 0.9057, 0.9228, 0.9187\n",
      "162, 0.9053, 0.9290, 0.9250\n",
      "163, 0.9055, 0.9228, 0.9187\n",
      "164, 0.9346, 0.9248, 0.9219\n",
      "165, 0.9055, 0.9186, 0.9125\n",
      "166, 0.9050, 0.9248, 0.9187\n",
      "167, 0.9142, 0.9207, 0.9156\n",
      "168, 0.9058, 0.9269, 0.9187\n",
      "169, 0.9049, 0.8935, 0.8969\n",
      "170, 0.9049, 0.9290, 0.9219\n",
      "171, 0.9050, 0.9311, 0.9250\n",
      "172, 0.9064, 0.9269, 0.9187\n",
      "173, 0.9056, 0.9248, 0.9187\n",
      "174, 0.9051, 0.9332, 0.9313\n",
      "175, 0.9123, 0.9353, 0.9313\n",
      "176, 0.9054, 0.9395, 0.9344\n",
      "177, 0.9051, 0.9374, 0.9344\n",
      "178, 0.9050, 0.9332, 0.9313\n",
      "179, 0.9050, 0.9374, 0.9344\n",
      "180, 0.9050, 0.9374, 0.9344\n",
      "181, 0.9051, 0.9395, 0.9344\n",
      "182, 0.9051, 0.9395, 0.9344\n",
      "183, 0.9052, 0.9395, 0.9344\n",
      "184, 0.9052, 0.9395, 0.9344\n",
      "185, 0.9053, 0.9395, 0.9344\n",
      "186, 0.9054, 0.9395, 0.9344\n",
      "187, 0.9055, 0.9395, 0.9344\n",
      "188, 0.9057, 0.9395, 0.9344\n",
      "189, 0.9058, 0.9395, 0.9344\n",
      "190, 0.9059, 0.9395, 0.9344\n",
      "191, 0.9059, 0.9395, 0.9344\n",
      "192, 0.9059, 0.9395, 0.9344\n",
      "193, 0.9059, 0.9395, 0.9344\n",
      "194, 0.9056, 0.9395, 0.9344\n",
      "195, 0.9056, 0.9395, 0.9344\n",
      "196, 0.9055, 0.9395, 0.9344\n",
      "197, 0.9056, 0.9395, 0.9344\n",
      "198, 0.9055, 0.9395, 0.9344\n",
      "199, 0.9054, 0.9395, 0.9344\n"
     ]
    }
   ],
   "source": [
    "### Classification ###\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    sae_model.parameters(), lr=learning_rate,momentum=0.9, weight_decay=1e-5)\n",
    "\n",
    "best_model_cls=sae_model\n",
    "prev_loss=10\n",
    "train_cost_history = []\n",
    "epoch=0\n",
    "\n",
    "while(True):\n",
    "    epoch+=1\n",
    "    for data in dataloader:\n",
    "        img, clas = data\n",
    "        img = img.view(img.size(0), -1)\n",
    "        img = Variable(img)\n",
    "        clas=clas.type(torch.LongTensor)\n",
    "        # ===================forward=====================\n",
    "        #print (img.shape)\n",
    "        output = sae_model(img)\n",
    "        loss = criterion(output, clas)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    delta_loss = prev_loss - loss.data\n",
    "    if dataset_no==1:\n",
    "        if epoch==200:  # convergence criteria gives bad results for dataset 1\n",
    "            break\n",
    "    else:\n",
    "        if delta_loss < convergence: # convergence gives good results for dataset 2\n",
    "            break\n",
    "    best_model_cls=sae_model\n",
    "    prev_loss=loss.data\n",
    "    train_cost_history.append(loss.data)\n",
    "\n",
    "    # ===================log========================\n",
    "    ## train acc ##\n",
    "    feat=Variable(features)\n",
    "    output=sae_model(feat)\n",
    "    targ=targets.type(torch.LongTensor)\n",
    "    tot=0\n",
    "    corr=0\n",
    "    #print (targ[0].item())\n",
    "    for i in range(targ.shape[0]):\n",
    "        in1=0\n",
    "        ma=0\n",
    "        for j in range(output.shape[1]):\n",
    "            if (output[i][j].item() > ma):\n",
    "                ma=output[i][j].item()\n",
    "                in1=j\n",
    "\n",
    "        if (targ[i].item() == in1):\n",
    "            corr=corr+1\n",
    "        tot=tot+1\n",
    "    train_acc=corr/tot\n",
    "    \n",
    "    ## test acc ##\n",
    "    feat=Variable(features_t)\n",
    "    output=sae_model(feat)\n",
    "    targ=targets_t.type(torch.LongTensor)\n",
    "    tot=0\n",
    "    corr=0\n",
    "    for i in range(targ.shape[0]):\n",
    "        in1=0\n",
    "        ma=0\n",
    "        for j in range(output.shape[1]):\n",
    "            if (output[i][j].item() > ma):\n",
    "                ma=output[i][j].item()\n",
    "                in1=j\n",
    "\n",
    "        if (targ[i].item() == in1):\n",
    "            corr=corr+1\n",
    "#         print (in1,targ[i].item())\n",
    "        tot=tot+1\n",
    "    test_acc=corr/tot\n",
    "    \n",
    "#     print('epoch {}, loss:{:.4f}, train_accuracy:{:.4f}, test_accuracy:{:.4f}'\n",
    "#           .format(epoch, loss.data, train_acc, test_acc))\n",
    "    print('{}, {:.4f}, {:.4f}, {:.4f}'\n",
    "          .format(epoch, loss.data, train_acc, test_acc))\n",
    "torch.save(best_model_cls.state_dict(), './sae_model_final_pretrained.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_model.load_state_dict(torch.load('./sae_model_final_pretrained.pth'))\n",
    "\n",
    "feat=Variable(features)\n",
    "output=sae_model(feat)\n",
    "pred = torch.argmax(output, 1)\n",
    "targ=targets.type(torch.LongTensor)\n",
    "\n",
    "cm = confusion_matrix(pred.view(-1), targ.view(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ ,coast ,forest ,highway ,street ,tallbuilding ,\n",
      "coast ,103.0000 , 1.0000 , 5.0000 , 1.0000 , 2.0000 , \n",
      "forest ,2.0000 , 86.0000 , 1.0000 , 0.0000 , 1.0000 , \n",
      "highway ,2.0000 , 0.0000 , 84.0000 , 2.0000 , 1.0000 , \n",
      "street ,1.0000 , 0.0000 , 1.0000 , 77.0000 , 0.0000 , \n",
      "tallbuilding ,1.0000 , 3.0000 , 0.0000 , 5.0000 , 100.0000 , \n"
     ]
    }
   ],
   "source": [
    "labels = ['coast','forest','highway','street','tallbuilding']\n",
    "\n",
    "print(\"_ ,\",end=\"\")\n",
    "for i in range(cm.shape[0]):\n",
    "    print(labels[i],',',end=\"\")\n",
    "print(\"\")\n",
    "for i in range(cm.shape[0]):\n",
    "    print(labels[i],',',end=\"\")\n",
    "    for j in range(cm.shape[1]):\n",
    "        print('{:.4f} , '.format(cm[i][j]),end = \"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
